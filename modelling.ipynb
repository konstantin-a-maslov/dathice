{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4413db7c-6fba-4a5e-a6f0-79fd941a7af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import pandas\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "210d65ab-8550-4a8c-8194-40a4cfd27507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pandas.read_csv(\"data/dataset.csv\")\n",
    "# keep only train/val data for modelling\n",
    "df = df[df.fold != -1]\n",
    "\n",
    "features = sorted([\n",
    "    \"H_modelled\",\n",
    "    \"H_modelled_grad\", \n",
    "    \"H_modelled_lapl\", \n",
    "    \"v_modelled\",\n",
    "    \"v_modelled_grad\",\n",
    "    \"v_modelled_lapl\",\n",
    "    \"boundary_proximity\",\n",
    "    \"elevation1\",\n",
    "    \"elevation1_gradient\", \n",
    "    \"elevation1_laplacian\",\n",
    "    \"elevation2_gradient\",\n",
    "    \"elevation2_laplacian\",\n",
    "    \"elevation_diff\",\n",
    "])\n",
    "\n",
    "feature_idxs = {feature: idx for idx, feature in enumerate(features)}\n",
    "target = \"H\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aa94b3c-1981-4766-b311-886eec647fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate min and max values of features \n",
    "feature_mins = {_: float(df[_].min()) for _ in features + [\"H\"]}\n",
    "feature_maxs = {_: float(df[_].max()) for _ in features + [\"H\"]}\n",
    "\n",
    "# some fixes to ensure consistency\n",
    "feature_mins[\"boundary_proximity\"] = 0.0\n",
    "feature_mins[\"H_modelled\"] = 0.0\n",
    "feature_mins[\"H\"] = 0.0\n",
    "feature_maxs[\"H\"] = feature_maxs[\"H_modelled\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7081dbfd-1ff7-4619-a2dc-e4144cf047a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save these stats to reuse later\n",
    "with open(\"data/stats.pickle\", \"wb\") as dst:\n",
    "    pickle.dump((feature_mins, feature_maxs), dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a2f636c-81ff-4829-8ea9-a1b382872ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise the data to be in range from 0 to 1\n",
    "df_norm = df.copy()\n",
    "\n",
    "for feature in features + [\"H\"]:\n",
    "    vmin = feature_mins[feature]\n",
    "    vmax = feature_maxs[feature]\n",
    "    feature_norm = (df_norm[feature] - vmin) / (vmax - vmin)\n",
    "    df_norm[feature] = feature_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce609d18-d371-4210-b7c8-c8bdd9c9e194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data generator\n",
    "def data_generator(df, batch_size, noise=0, rng_key=None):\n",
    "    if noise and rng_key is None:\n",
    "        raise ValueError(\"rng_key should be provided if noise is used\")\n",
    "    if rng_key is not None and isinstance(rng_key, int):\n",
    "        rng_key = jax.random.PRNGKey(rng_key)\n",
    "    df = df.copy()\n",
    "    df = df.sample(frac=1).reset_index() # shuffle\n",
    "    for idx in range(0, len(df), batch_size):\n",
    "        batch = df.iloc[idx:min(idx + batch_size, len(df))]\n",
    "        x, y = convert_to_jax_xy_pair(batch)\n",
    "        if noise:\n",
    "            for feature in x:\n",
    "                rng_key, new_rng_key = jax.random.split(rng_key, 2)\n",
    "                feature_noise = jax.random.normal(rng_key, x[feature].shape) * noise\n",
    "                x[feature] = x[feature] + feature_noise\n",
    "                rng_key = new_rng_key\n",
    "            x[\"boundary_proximity\"] = jnp.clip(x[\"boundary_proximity\"], a_min=0.0)\n",
    "        yield x, y\n",
    "\n",
    "\n",
    "def convert_to_jax_xy_pair(df, features=features, target=target):\n",
    "    x = {_: jnp.array(df[_]) for _ in features}\n",
    "    y = jnp.array(df[target])    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633a170d-af5c-4d77-b7d3-08fbaae9870b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "397ce99c-1297-4b05-bd49-8a7d0b46f0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a general multi-layer perceptron\n",
    "def initialise_mlp_params(\n",
    "    in_size, out_size, n_hidden_layers, n_hidden_units, \n",
    "    initialiser=jax.nn.initializers.he_uniform(),\n",
    "    rng_key=jax.random.PRNGKey(42)\n",
    "):\n",
    "    params = []\n",
    "    \n",
    "    for l_idx, _ in enumerate(range(n_hidden_layers + 1)):\n",
    "        key_w, rng_key = jax.random.split(rng_key, 2)\n",
    "        if n_hidden_layers == 0:\n",
    "            w_shape = (out_size, in_size)\n",
    "            b_shape = out_size\n",
    "        elif l_idx == 0:\n",
    "            w_shape = (n_hidden_units, in_size)\n",
    "            b_shape = n_hidden_units\n",
    "        elif l_idx == n_hidden_layers:\n",
    "            w_shape = (out_size, n_hidden_units)\n",
    "            b_shape = out_size\n",
    "        else:\n",
    "            w_shape = (n_hidden_units, n_hidden_units)\n",
    "            b_shape = n_hidden_units\n",
    "        w = initialiser(key_w, w_shape)\n",
    "        b = jnp.zeros(b_shape)\n",
    "        params.append((w, b))\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def build_mlp(activation, final_activation=None, squeeze=False):\n",
    "    if final_activation is None:\n",
    "        def linear(x):\n",
    "            return x\n",
    "        final_activation = jax.jit(linear)\n",
    "        \n",
    "    def mlp(params, x):\n",
    "        for w, b in params[:-1]:\n",
    "            x = jnp.dot(w, x) + b\n",
    "            x = activation(x)\n",
    "        w, b = params[-1]\n",
    "        y = jnp.dot(w, x) + b\n",
    "        y = final_activation(y)\n",
    "        if squeeze:\n",
    "            y = jnp.squeeze(y)\n",
    "        return y\n",
    "\n",
    "    return jax.jit(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee718330-da0e-4fa3-8aa6-40344d2b86a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define DATHICE model which is boundary_proximity * f1(x) * ReLU(H_modelled + f2(x))\n",
    "def build_dathice(mlp_bcs, mlp):\n",
    "    def dathice(params, x):\n",
    "        mlp_bcs_params, mlp_params = params\n",
    "        p = jnp.array([x[_] for _ in sorted(x.keys())])\n",
    "        mlp_bcs_output = mlp_bcs(mlp_bcs_params, p)\n",
    "        mlp_output = mlp(mlp_params, p)\n",
    "        d = x[\"boundary_proximity\"]\n",
    "        H_modelled = x[\"H_modelled\"]\n",
    "        bcs_factor = d * mlp_bcs_output\n",
    "        H_corrected = H_modelled + mlp_output\n",
    "        H_constrained = jax.nn.relu(H_corrected)\n",
    "        y = bcs_factor * H_constrained\n",
    "        return y\n",
    "        \n",
    "    return jax.jit(dathice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7da211-6e7b-4d2b-a1ad-9f683be4bdb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aadfd5f7-b708-4904-aa0e-e000df81cd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss functions\n",
    "@jax.jit\n",
    "def ae(true, pred):\n",
    "    return jnp.abs(true - pred)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def se(true, pred):\n",
    "    return (true - pred)**2.0\n",
    "    \n",
    "    \n",
    "@jax.jit\n",
    "def logcosh(true, pred):\n",
    "    return jnp.log(jnp.cosh(true - pred))\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def sle(true, pred): # only for positive values, which works here\n",
    "    return (jnp.log(true + 1.0) - jnp.log(pred + 1.0))**2.0\n",
    "\n",
    "\n",
    "def mean_aggregate(loss_metric):\n",
    "    @jax.jit\n",
    "    def aggregated(true, pred):\n",
    "        loss_values = loss_metric(true, pred)\n",
    "        return jnp.mean(loss_values)\n",
    "    return aggregated\n",
    "\n",
    "\n",
    "def scale_loss_metric(loss_metric):\n",
    "    EPSILON = 1e-3\n",
    "    @jax.jit\n",
    "    def scaled_loss_metric(true, pred):\n",
    "        loss_values = loss_metric(true, pred)\n",
    "        loss_scales = loss_metric(true, 0) + EPSILON\n",
    "        return loss_values / loss_scales\n",
    "    return scaled_loss_metric\n",
    "\n",
    "\n",
    "def combine_loss_metrics(loss_metrics, weights=None):\n",
    "    if weights is None:\n",
    "        weights = [1.0 for _ in loss_metrics]\n",
    "    if len(loss_metrics) != len(weights):\n",
    "        raise ValueError(\"The lengths of loss_metrics and weights should be equal.\")\n",
    "    @jax.jit\n",
    "    def combined_loss_metric(true, pred):\n",
    "        total = 0\n",
    "        for loss_metric, weight in zip(loss_metrics, weights):\n",
    "            total += (weight * loss_metric(true, pred))\n",
    "        return total\n",
    "    return combined_loss_metric\n",
    "    \n",
    "\n",
    "def get_loss_fn(batched_forward, loss_metric, aggregate=mean_aggregate):\n",
    "    @jax.jit\n",
    "    def loss_fn(params, x, true):\n",
    "        pred = batched_forward(params, x)\n",
    "        loss_value = aggregate(loss_metric)(true, pred)\n",
    "        return loss_value\n",
    "    return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecece18b-a907-4454-8247-08f265d3acf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0147622-a80b-4d67-902a-537a9a08835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whether to perform hyperparameter tuning or not\n",
    "hyperparameter_tuning = False\n",
    "\n",
    "if not hyperparameter_tuning:\n",
    "    # a mock hyperparameter set\n",
    "    hyperparameters = dict(\n",
    "        activation_function=\"elu\",\n",
    "        nlayers1=4,\n",
    "        nlayers2=4,\n",
    "        nneurons1=8,\n",
    "        nneurons2=8,\n",
    "        init_method=\"lecun\",\n",
    "        loss=\"se\",\n",
    "        batch_size=128,\n",
    "        grad_norm_clip=3,\n",
    "        l2_regularization=0,\n",
    "        base_lr=0.25,\n",
    "        data_noise=1e-2,\n",
    "        optimizer=\"sgd\", \n",
    "        nepochs=500,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    \"\"\"\n",
    "    EXERCISE: Implement hyperparameter tuning\n",
    "    HINT: You might want to reuse the code below to construct a cost function\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048b3faf-3d22-4153-80d9-503474845efd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d22171d-9c0d-43cf-88ef-76cf808ffb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build and train the final model\n",
    "activation_fuction = {\n",
    "    \"swish\": jax.nn.swish,\n",
    "    \"relu\": jax.nn.relu,\n",
    "    \"softplus\": jax.nn.softplus,\n",
    "    \"elu\": jax.nn.elu,\n",
    "}[hyperparameters[\"activation_function\"]]\n",
    "\n",
    "init_method = {\n",
    "    \"he\": jax.nn.initializers.he_uniform,\n",
    "    \"glorot\": jax.nn.initializers.glorot_uniform,\n",
    "    \"lecun\": jax.nn.initializers.lecun_uniform,\n",
    "}[hyperparameters[\"init_method\"]]\n",
    "\n",
    "# boundary-condition MLP\n",
    "mlp_bcs = build_mlp(\n",
    "    activation_fuction, \n",
    "    final_activation=jax.nn.softplus, \n",
    "    squeeze=True\n",
    ")\n",
    "# correction MLP\n",
    "mlp = build_mlp(\n",
    "    activation_fuction, \n",
    "    squeeze=True\n",
    ")\n",
    "\n",
    "dathice = build_dathice(mlp_bcs, mlp)\n",
    "\n",
    "mlp_bcs_batched = jax.vmap(mlp_bcs, in_axes=(None, 0))\n",
    "mlp_batched = jax.vmap(mlp, in_axes=(None, 0))\n",
    "dathice_batched = jax.vmap(dathice, in_axes=(None, 0))\n",
    "\n",
    "mlp_bcs_params = initialise_mlp_params(\n",
    "    len(features), \n",
    "    1, \n",
    "    hyperparameters[\"nlayers1\"], \n",
    "    hyperparameters[\"nneurons1\"], \n",
    "    init_method()\n",
    ")\n",
    "mlp_params = initialise_mlp_params(\n",
    "    len(features), \n",
    "    1, \n",
    "    hyperparameters[\"nlayers2\"], \n",
    "    hyperparameters[\"nneurons2\"], \n",
    "    init_method()\n",
    ")\n",
    "\n",
    "dathice_params = (mlp_bcs_params, mlp_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26e9f62c-33c7-4a3e-9999-4c821833c285",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = hyperparameters[\"base_lr\"] * np.sqrt(hyperparameters[\"batch_size\"] / 32)\n",
    "\n",
    "# in optax, optimizers are defined as operations on gradients\n",
    "opt_chain = [\n",
    "    optax.clip_by_global_norm(hyperparameters[\"grad_norm_clip\"]), \n",
    "] + [optax.scale_by_adam()] if hyperparameters[\"optimizer\"] == \"adam\" else [] + \\\n",
    "    [optax.scale_by_rms()] if hyperparameters[\"optimizer\"] == \"rms\" else [] + [\n",
    "    optax.add_decayed_weights(hyperparameters[\"l2_regularization\"]),\n",
    "    optax.scale(-lr), # !note the minus sign\n",
    "]\n",
    "optimizer = optax.chain(*opt_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ad95f91-e281-49f8-88d2-dff05bcf7f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss = 0.006475: 100%|██████████| 500/500 [02:08<00:00,  3.88it/s]\n"
     ]
    }
   ],
   "source": [
    "loss = {\n",
    "    \"ae\": ae,\n",
    "    \"se\": se,\n",
    "    \"logcosh\": logcosh,\n",
    "    \"sle\": sle,\n",
    "}[hyperparameters[\"loss\"]]\n",
    "\n",
    "params = dathice_params\n",
    "\n",
    "opt_state = optimizer.init(params)\n",
    "loss_fn = get_loss_fn(dathice_batched, loss)\n",
    "loss_fn_grad = jax.value_and_grad(loss_fn)\n",
    "\n",
    "noise_key = jax.random.PRNGKey(42)\n",
    "\n",
    "\n",
    "# one training step (i.e., for one batch)\n",
    "@jax.jit\n",
    "def step(params, x, y, opt_state):\n",
    "    loss, grads = loss_fn_grad(params, x, y)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss, grads\n",
    "\n",
    "\n",
    "with tqdm(total=hyperparameters[\"nepochs\"], desc=\"\") as pbar:\n",
    "    for epoch in range(hyperparameters[\"nepochs\"]):             \n",
    "        noise_key, new_noise_key = jax.random.split(noise_key, 2)\n",
    "    \n",
    "        train_loss = 0\n",
    "        n_steps = 0\n",
    "\n",
    "        # here (assuming after hyperparameter tuning), we merge train and validation subsets\n",
    "        for x, y in data_generator(\n",
    "            df_norm, \n",
    "            hyperparameters[\"batch_size\"], \n",
    "            noise=hyperparameters[\"data_noise\"], \n",
    "            rng_key=noise_key\n",
    "        ):\n",
    "            params, opt_state, loss, grads = step(params, x, y, opt_state)\n",
    "            train_loss += loss\n",
    "            n_steps += 1\n",
    "        train_loss /= n_steps\n",
    "            \n",
    "        noise_key = new_noise_key\n",
    "\n",
    "        pbar.set_description(f\"Train loss = {train_loss:.6f}\")\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68c83610-76ba-495e-8884-b9c9facd9c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the parameters and hyperparameters\n",
    "# adjust the paths as needed\n",
    "with open(\"params/params.pickle\", \"wb\") as dst:\n",
    "    pickle.dump(params, dst)\n",
    "with open(\"hyperparameters/hyperparameters.pickle\", \"wb\") as dst:\n",
    "    pickle.dump(hyperparameters, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20714287-451d-41ae-bed9-4a1e484cafcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
